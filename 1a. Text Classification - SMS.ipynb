{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. Text Classification - SMS.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":["rKqe-5Lpwj7E","c2h-db6Zw00i","pERZB-yYcMnQ","8_WO6pSdvE3o","Pn-BuPr7wj7u","4o12llmedMEi","0fAMugIPwj-9","YIHaWAagwj_H"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rKqe-5Lpwj7E","colab_type":"text"},"source":["### 1. SMS Data Exploration"]},{"cell_type":"markdown","metadata":{"id":"SgZQ20psaJJX","colab_type":"text"},"source":["SMS data is available as CSV file along with class material. In the code below, we are copying the from Google drive."]},{"cell_type":"code","metadata":{"id":"evvJtcnyaeO_","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o84tU4KBruJK","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WcQh6U1-aeOh","colab_type":"code","colab":{}},"source":["# read file into pandas using a relative path. Please change the path as needed\n","sms_df = pd.read_table('/gdrive/My Drive/Great Learning/Statistical NLP/Notebooks/data/sms.tsv', header=None, names=['label', 'message'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZOgLs2FvbRx-","colab_type":"code","colab":{}},"source":["#Total number of SMS\n","sms_df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qDN-bzy9aBAz","colab_type":"code","colab":{}},"source":["#Check the contents of dataframe\n","sms_df.sample(n=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EAimr_7Trmk0","colab_type":"code","colab":{}},"source":["#Spam vs ham\n","sms_df.groupby('label').count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4AtiyLOW0Fp7","colab_type":"code","colab":{}},"source":["4825/5572"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MvIX0dRgbbzp","colab_type":"code","colab":{}},"source":["#Check out SMS messages which is legitimate - ham\n","msg_num = np.random.randint(0, sms_df.shape[0])\n","print(sms_df.loc[msg_num, 'label'], ':', sms_df.loc[msg_num, 'message'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EGvWX0DvY48J","colab_type":"code","colab":{}},"source":["#Check out SMS messages which is a SPAM\n","print(sms_df.loc[1734, 'label'], ':', sms_df.loc[1734, 'message'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A705u9O1uor6","colab_type":"code","colab":{}},"source":["#Checkout missing values\n","sms_df.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oy8Dja0ExL-B","colab_type":"code","colab":{}},"source":["# convert label to a numerical variable\n","sms_df['label_num'] = sms_df.label.map({'ham':0, 'spam':1})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HBd1Q7HJxOox","colab_type":"code","colab":{}},"source":["#We should have label_num column in dataframe\n","sms_df.sample(n=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c2h-db6Zw00i","colab_type":"text"},"source":["### 2. Create Training & Test Dataset"]},{"cell_type":"code","metadata":{"id":"v-wdhCspw4lE","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4mY8_xP6w9DW","colab_type":"code","colab":{}},"source":["# split X and y into training and testing sets\n","sms_train, sms_test, y_train, y_test = train_test_split(sms_df.message, sms_df.label_num, random_state=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"axydR7nwxd5c","colab_type":"code","colab":{}},"source":["#Traing data\n","print(sms_train.shape)\n","print(y_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cMpz2JpNxjT1","colab_type":"code","colab":{}},"source":["#Test Data\n","print(sms_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pERZB-yYcMnQ","colab_type":"text"},"source":["### 3. Tokenization & Vectorization"]},{"cell_type":"markdown","metadata":{"id":"HG5KU5JVwj7k","colab_type":"text"},"source":["Using **CountVectorizer**, to get numeric features."]},{"cell_type":"code","metadata":{"id":"o-EAh3Lict_N","colab_type":"code","colab":{}},"source":["# import and instantiate CountVectorizer (with the default parameters)\n","from sklearn.feature_extraction.text import CountVectorizer\n","cvect = CountVectorizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOIKxD1pdEg3","colab_type":"code","colab":{}},"source":["#Feed SMS data to CountVectorizer\n","cvect.fit(sms_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CF-9vF0ZxQO6","colab_type":"code","colab":{}},"source":["#Check the vocablury size\n","len(cvect.vocabulary_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H4t9gegJd74n","colab_type":"code","colab":{}},"source":["#What is there in the vocabulary\n","cvect.vocabulary_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9HcJcr59weGX","colab_type":"code","colab":{}},"source":["cvect.get_feature_names()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UK1zjGfHuP4U","colab_type":"text"},"source":["Build Document-term Matrix (DTM)"]},{"cell_type":"code","metadata":{"id":"DrsvbF1XdefF","colab_type":"code","colab":{}},"source":["#Convert Training SMS messages into Count Vectors\n","X_train_ct = cvect.transform(sms_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qDPMLuFduAx","colab_type":"code","colab":{}},"source":["#Size of Document Term Matrix\n","X_train_ct.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8vDpvDjPxi7c","colab_type":"code","colab":{}},"source":["sms_train[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FR2qtdrMeXkL","colab_type":"code","colab":{}},"source":["#Let's check the first record\n","X_train_ct[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YaTnzUIMeoMt","colab_type":"code","colab":{}},"source":["#What's there in sparse matrix\n","print(X_train_ct[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"twoJfGyXwj80","colab_type":"text"},"source":["From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n","\n","> As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have **many feature values that are zeros** (typically more than 99% of them).\n","\n","> For instance, a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.\n","\n","> In order to be able to **store such a matrix in memory** but also to **speed up operations**, implementations will typically use a **sparse representation** such as the implementations available in the `scipy.sparse` package."]},{"cell_type":"markdown","metadata":{"id":"9zo2EwHYzAUr","colab_type":"text"},"source":["Convert Test SMS also in numerical features"]},{"cell_type":"code","metadata":{"id":"HX8ht6-tzDuu","colab_type":"code","colab":{}},"source":["X_test_ct = cvect.transform(sms_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zWjwfp9AzKL_","colab_type":"code","colab":{}},"source":["X_test_ct.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8_WO6pSdvE3o","colab_type":"text"},"source":["### 4. Building an SMS Classifier"]},{"cell_type":"markdown","metadata":{"id":"HEYiNTZPx_UJ","colab_type":"text"},"source":["Let's first try K-Nearest Neigbour algorithm"]},{"cell_type":"code","metadata":{"id":"f4ysA2I8wj7l","colab_type":"code","colab":{}},"source":["from sklearn.neighbors import KNeighborsClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"41v1PorCyHHj","colab_type":"code","colab":{}},"source":["# instantiate the model (with the default parameters)\n","knn = KNeighborsClassifier()\n","\n","# fit the model with data (occurs in-place)\n","knn.fit(X_train_ct, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lzexfE7Wwj7p","colab_type":"text"},"source":["Evaluation on Test Dataset"]},{"cell_type":"code","metadata":{"id":"uQOrAY7gy2Gp","colab_type":"code","colab":{}},"source":["from sklearn import metrics"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zjw9Iw7Yz2za","colab_type":"code","colab":{}},"source":["#Calculate accuracy on Training Dataset\n","metrics.accuracy_score(y_train, knn.predict(X_train_ct))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6jJ1fLXAwj7q","colab_type":"code","colab":{}},"source":["#Calculate accuracy on Test Dataset\n","metrics.accuracy_score(y_test, knn.predict(X_test_ct))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hyJmh5BuzZhW","colab_type":"text"},"source":["We can build Classifier using other algorithms e.g SVM"]},{"cell_type":"code","metadata":{"id":"G0k6aBRTzY6i","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVC"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DxLV-8oZbHQs","colab_type":"code","colab":{}},"source":["#Train an SVM with default parameters\n","svc = SVC()\n","svc.fit(X_train_ct, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Oq4hVfHbWM0","colab_type":"code","colab":{}},"source":["#Calculate accuracy on Test Dataset\n","metrics.accuracy_score(y_test, svc.predict(X_test_ct))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pn-BuPr7wj7u","colab_type":"text"},"source":["### 5. Using TF-IDF Vectorizer"]},{"cell_type":"code","metadata":{"id":"_h06uXvYwj7u","colab_type":"code","colab":{}},"source":["# import and instantiate TF-IDF Vectorizer (with the default parameters)\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","tvect = TfidfVectorizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8bAGvD0Ewj7y","colab_type":"code","colab":{}},"source":["#Feed SMS data to CountVectorizer\n","tvect.fit(sms_train)\n","\n","#Check the vocablury size\n","len(tvect.vocabulary_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"exeOGALacCb4","colab_type":"code","colab":{}},"source":["#Convert Training SMS messages into numerical values\n","X_train_tfidf = tvect.transform(sms_train)\n","\n","X_train_tfidf.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VcJwt_UscVWO","colab_type":"code","colab":{}},"source":["#Check first example\n","print(X_train_tfidf[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0vy_edXec0vP","colab_type":"code","colab":{}},"source":["#Convert Test SMSes also to tf-idf vectors\n","X_test_tfidf = tvect.transform(sms_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-c77Chsvcg5d","colab_type":"text"},"source":["Build an SVM"]},{"cell_type":"code","metadata":{"id":"eGxYIu00ca-s","colab_type":"code","colab":{}},"source":["svc_tf = SVC()\n","svc_tf.fit(X_train_tfidf, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AeXoA575cr3A","colab_type":"code","colab":{}},"source":["#Calculate accuracy on Test Dataset\n","metrics.accuracy_score(y_test, svc_tf.predict(X_test_tfidf))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4o12llmedMEi","colab_type":"text"},"source":["### 6. TF-IDF with ngram"]},{"cell_type":"code","metadata":{"id":"JTfCp4DK0pOE","colab_type":"code","colab":{}},"source":["sms_train[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XJnl3-kXdPuT","colab_type":"code","colab":{}},"source":["#Use ngrams of length upto 2 words\n","tvect_ngram = TfidfVectorizer(ngram_range=(1,2)) #Tokens can be made of 1 word or 2 words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"amKZ3BmGdr4x","colab_type":"code","colab":{}},"source":["#Feed SMS data to CountVectorizer\n","tvect_ngram.fit(sms_train)\n","\n","#Check the vocablury size\n","len(tvect_ngram.vocabulary_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xVr8GXs91ME_","colab_type":"text"},"source":["The movie was awesome\n","\n","Words as tokens = \"The\", \"movie\", \"was\", awesome\"\n","\n","ngrams (1,2) -> \"The\", \"movie\", \"was\", awesome\", \"The movie\", \"movie was\", \"was awesome\""]},{"cell_type":"code","metadata":{"id":"t0gI6RN-dz3i","colab_type":"code","colab":{}},"source":["tvect_ngram.vocabulary_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3__SHIJeCKI","colab_type":"code","colab":{}},"source":["#Convert Training SMS messages into numerical values\n","X_train_tfidf_ngram = tvect_ngram.transform(sms_train)\n","\n","X_train_tfidf_ngram.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7nxDtrUVN0R","colab_type":"code","colab":{}},"source":["svc_tf = SVC()\n","svc_tf.fit(X_train_tfidf_ngram, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"epvazfQbVYGu","colab_type":"code","colab":{}},"source":["#Calculate accuracy on Test Dataset\n","metrics.accuracy_score(y_test, svc_tf.predict(tvect_ngram.transform(sms_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bB6X2lLTwj9F","colab_type":"text"},"source":["**Summary:**\n","\n","- `vect.fit(train)` **learns the vocabulary** of the training data\n","- `vect.transform(train)` uses the **fitted vocabulary** to build a document-term matrix from the training data\n","- `vect.transform(test)` uses the **fitted vocabulary** to build a document-term matrix from the testing data and **ignores tokens** it hasn't seen before"]},{"cell_type":"markdown","metadata":{"id":"rE6sCTxjhPxR","colab_type":"text"},"source":["### 7. Building a Deep Learning Model"]},{"cell_type":"code","metadata":{"id":"pXcQt_TchaNW","colab_type":"code","colab":{}},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MofkwztZheaj","colab_type":"text"},"source":["We will use CountVectorizer features in this case. This can be replaced by TF-IDF features"]},{"cell_type":"code","metadata":{"id":"Qwt45bfbhcur","colab_type":"code","colab":{}},"source":["#Start building a Keras Sequential Model\n","tf.keras.backend.clear_session()\n","model = tf.keras.Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jmYjSF9Mh11J","colab_type":"code","colab":{}},"source":["#Add hidden layers\n","model.add(tf.keras.layers.Dense(100, activation='relu', input_shape=(len(tvect.vocabulary_),)))\n","model.add(tf.keras.layers.Dropout(0.4))\n","model.add(tf.keras.layers.Dense(50, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.4))\n","\n","#Add Output layer\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MUPy_5VIiUG9","colab_type":"code","colab":{}},"source":["#Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nigCTrwjXeWM","colab_type":"code","colab":{}},"source":["X_train_ct.todense()[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cn8kdNu3YUek","colab_type":"code","colab":{}},"source":["print(X_train_ct[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zH-LnFyXiXqc","colab_type":"code","colab":{}},"source":["model.fit(X_train_tfidf.todense(), y_train,\n","           validation_data=(X_test_tfidf.todense(), y_test), \n","           epochs=10, batch_size=32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0fAMugIPwj-9","colab_type":"text"},"source":["### 8. Controlling Vocabulary size\n","\n","Thus far, we have been using the default parameters of [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html):"]},{"cell_type":"code","metadata":{"id":"9CbGtaqRwj--","colab_type":"code","colab":{}},"source":["# show default parameters for CountVectorizer (TFIDF will have similar parameters)\n","cvect"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AWehIREdwj-_","colab_type":"text"},"source":["However, the vectorizer is worth tuning, just like a model is worth tuning! Here are a few parameters that you might want to tune:\n","\n","- **stop_words:** string {'english'}, list, or None (default)\n","    - If 'english', a built-in stop word list for English is used.\n","    - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n","    - If None, no stop words will be used."]},{"cell_type":"code","metadata":{"id":"DkCZFIKFwj_A","colab_type":"code","colab":{}},"source":["# remove English stop words\n","vect = CountVectorizer(stop_words='english')\n","vect.fit(sms_train)\n","len(vect.get_feature_names())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZAhoUBLRwj_A","colab_type":"text"},"source":["- **ngram_range:** tuple (min_n, max_n), default=(1, 1)\n","    - The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n","    - All values of n such that min_n <= n <= max_n will be used."]},{"cell_type":"code","metadata":{"id":"zvexuCwFwj_B","colab_type":"code","colab":{}},"source":["# include 1-grams, 2-grams and 3-grams\n","vect = CountVectorizer(ngram_range=(1, 3))\n","vect.fit(sms_train)\n","len(vect.get_feature_names())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzM7sYXHwj_D","colab_type":"text"},"source":["- **max_df:** float in range [0.0, 1.0] or int, default=1.0\n","    - When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n","    - If float, the parameter represents a proportion of documents.\n","    - If integer, the parameter represents an absolute count."]},{"cell_type":"code","metadata":{"id":"5QB_5WAnwj_D","colab_type":"code","colab":{}},"source":["# ignore terms that appear in more than 50% of the documents\n","vect = CountVectorizer(max_df=0.5)\n","vect.fit(sms_train)\n","len(vect.get_feature_names())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0lPcHFE1s8Aa","colab_type":"text"},"source":["- **min_df:** int, default=1\n","\n","\n","> Defines, at a minimum, how many documents a word should appear before it is included in Vocablury\n"]},{"cell_type":"code","metadata":{"id":"vHC-oizLwj_F","colab_type":"code","colab":{}},"source":["# only keep terms that appear in at least 2 documents\n","vect = CountVectorizer(min_df=2)\n","vect.fit(sms_train)\n","len(vect.get_feature_names())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4FsdJ9FBEIlY","colab_type":"code","colab":{}},"source":["vect.get_feature_names()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bsmJkEbptdto","colab_type":"text"},"source":["- **max_features**: int or None, default=None\n","\n","\n","> Maximum size of vocabulary. None means no hard limit.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"AV31mVnws27q","colab_type":"code","colab":{}},"source":["# only keep terms that appear in at least 2 documents, but maximum vocablury is restricted to 2000 words\n","vect = CountVectorizer(min_df=4, max_features=2000)\n","vect.fit(sms_train)\n","len(vect.get_feature_names())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9RREbq_xwj_G","colab_type":"text"},"source":["**Guidelines for tuning Vectorizer:**\n","\n","- Use your knowledge of the **problem** and the **text**\n","- **Experiment**, and let the data tell you the best approach!\n","- Quiet often, number of features are limited by amount of RAM/Compute available."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"YIHaWAagwj_H","colab_type":"text"},"source":["### Word Cloud"]},{"cell_type":"code","metadata":{"id":"SAK6X0-jwj_H","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt # visualization\n","from wordcloud import WordCloud "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5ix7Va-wj_M","colab_type":"code","colab":{}},"source":["# Define wordcloud function from wordcloud library.\n","wc = WordCloud()\n","wc.generate(str(sms_df['message']))\n","# declare our figure \n","plt.figure(figsize=(20,10), facecolor='k')\n","# add title to the graph\n","plt.title(\"Most frequent words in SMS dataset\", fontsize=40, color='white')\n","plt.imshow(wc)\n","plt.show()"],"execution_count":null,"outputs":[]}]}