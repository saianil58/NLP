{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Working with Web Data.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":["qgy1Jzg-wyRp","dVJ8LqgK1Q4w","ZPCzgz2ZwySW","NBK3ZgwUwySt","k2c5hDSNwyTw","MpmBJJXhwyT2","fPr-C-Q1n3pE"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qgy1Jzg-wyRp"},"source":["### Web data retrieval"]},{"cell_type":"code","metadata":{"id":"-Z1V23urwyRr"},"source":["import requests\n","import textwrap"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Chgw6qBTwyRv"},"source":["#Let's download the data from inshorts website. In this case, news articles will be from 'technolgy' category\n","url = 'https://inshorts.com/en/read/technology'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"21pavZKVwyR1"},"source":["news_category = url.split('/')[-1]\n","news_category"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gqanoc8o1NCn"},"source":["Download HTML data"]},{"cell_type":"code","metadata":{"id":"q9xSbV5BwyR-"},"source":["#Download the data from Website\n","data = requests.get(url)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R2NaLao-gBtD"},"source":["data.content"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NNu23gpBGJxm"},"source":["Data Cleaning - We can use Beautiful Soup package to clean Web data"]},{"cell_type":"code","metadata":{"id":"K87WWgtuGIcL"},"source":["from bs4 import BeautifulSoup"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZ8gHfTBwySC"},"source":["soup = BeautifulSoup(data.content, 'html.parser')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l4wI2Dqez5Xh"},"source":["soup"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mi3FrilR1ZbH"},"source":["Read all the articles. For each article, we will read:\n","\n","1. Headline\n","2. Article body\n","3. Category\n","\n","This is done by reading text between specific HTML tags. The tags depend on actual web page"]},{"cell_type":"code","metadata":{"id":"Y6NOaNg-wyRy"},"source":["news_data = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qcmvcnDfwySF"},"source":["news_articles = [{'news_headline': headline.find('span', attrs={'itemprop': 'headline'}).string,\n","                  'news_article': article.find('div', attrs={'itemprop': 'articleBody'}).string,\n","                  'news_category': news_category} \n","                 for headline, article in zip(soup.find_all('div', \n","                                                            class_ = ['news-card-title news-right-box']), \n","                                              soup.find_all('div', class_=['news-card-content news-right-box']))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mkrtuxs2wySJ"},"source":["#Check news data\n","news_data.extend(news_articles)\n","news_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pEnaJDntGY15"},"source":["Read the news data in a Dataframe"]},{"cell_type":"code","metadata":{"id":"ZPlHoL7GGcgk"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GjsAGXcEwySN"},"source":["#Building dataframe\n","df = pd.DataFrame(news_data, columns=['news_headline', 'news_article', 'news_category'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QPebjVhOwyST"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZPCzgz2ZwySW"},"source":["### Extract more Web data"]},{"cell_type":"markdown","metadata":{"id":"2xPXI0_R1hNh"},"source":["\n","Function to extract data from inshorts.com. The function will:\n","\n","1. Take a URLs list as input\n","2. Get content for each URL\n","3. Extract news article headline, body and category"]},{"cell_type":"code","metadata":{"id":"XfwI4DfwwySX"},"source":["urls_list = ['https://inshorts.com/en/read/technology',\n","             'https://inshorts.com/en/read/sports',\n","             'https://inshorts.com/en/read/world']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jitD63HQwySa"},"source":["def datasetPrepare(urls_list):\n","    \n","    news_data = []\n","    for url in urls_list:\n","        news_category = url.split('/')[-1]\n","        data = requests.get(url)\n","        soup = BeautifulSoup(data.content, 'html.parser')\n","        news_articles = [{'news_headline': headline.find('span', attrs={\"itemprop\": \"headline\"}).string,\n","                          'news_article': article.find('div', attrs={\"itemprop\": \"articleBody\"}).string,\n","                          'news_category': news_category}\n","                         \n","                            for headline, article in \n","                             zip(soup.find_all('div', class_=[\"news-card-title news-right-box\"]),\n","                                 soup.find_all('div', class_=[\"news-card-content news-right-box\"]))\n","                        ]\n","        news_data.extend(news_articles) \n","    df =  pd.DataFrame(news_data)\n","    df = df[['news_headline', 'news_article', 'news_category']]\n","    return df    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ruY2Z-DIwySd"},"source":["#Build the dataframe\n","news_df = datasetPrepare(urls_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCfWltFHwySm"},"source":["news_df.sample(n=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rXVQKGNwySp"},"source":["#Articles count by category\n","news_df.news_category.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NBK3ZgwUwySt"},"source":["### Text Preprocessing"]},{"cell_type":"code","metadata":{"id":"tPQmV8DiJJF1"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tXQNHEHpJKdf"},"source":["import re\n","import unicodedata\n","from nltk.stem import WordNetLemmatizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wh9CrhePwyS9"},"source":["Remove HTML tags"]},{"cell_type":"code","metadata":{"id":"1Xit3_ckwyS9"},"source":["def strip_html_tags(text):\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    stripped_text = soup.get_text()\n","    return stripped_text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HBbRSoPkwyTB"},"source":["Remove accented characters"]},{"cell_type":"code","metadata":{"id":"TOBdIlTWwyTB"},"source":["def remove_accented_chars(text):\n","    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8u9SuUXOwyTM"},"source":["Remove special characters"]},{"cell_type":"code","metadata":{"id":"lCmBJeI5wyTN"},"source":["def remove_special_characters(text, remove_digits=False):\n","    #Using regex\n","    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n","    text = re.sub(pattern, '', text)\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ryt_VSOVwyTT"},"source":["Lemmatization"]},{"cell_type":"code","metadata":{"id":"uNR0fNT4wyTU"},"source":["def lemmatize_text(text):\n","\n","    lemmatizer = WordNetLemmatizer()\n","    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jWmDYl9swyTa"},"source":["Stemming"]},{"cell_type":"code","metadata":{"id":"wcgdJFzLwyTb"},"source":["def simple_stemmer(text):\n","    ps = nltk.porter.PorterStemmer()\n","    text = ' '.join([ps.stem(word) for word in text.split()])\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S8AdJuUuwyTq"},"source":["#### Building a text normalizer"]},{"cell_type":"code","metadata":{"id":"brUdlUOJwyTu"},"source":["def normalize_corpus(corpus, html_stripping=True, accented_char_removal=True, text_lower_case=True, \n","                     text_lemmatization=True, special_char_removal=True, \n","                     stopword_removal=True, remove_digits=True):\n","    \n","    normalized_corpus = []\n","    # normalize each document in the corpus\n","    for doc in corpus:\n","        # strip HTML\n","        if html_stripping:\n","            doc = strip_html_tags(doc)\n","        # remove accented characters\n","        if accented_char_removal:\n","            doc = remove_accented_chars(doc)\n","        # lowercase the text    \n","        if text_lower_case:\n","            doc = doc.lower()\n","        # remove extra newlines\n","        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n","        # lemmatize text\n","        if text_lemmatization:\n","            doc = lemmatize_text(doc)\n","        # remove special characters and\\or digits    \n","        if special_char_removal:\n","            # insert spaces between special characters to isolate them    \n","            special_char_pattern = re.compile(r'([{.(-)!}])')\n","            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n","            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n","        # remove extra whitespace\n","        doc = re.sub(' +', ' ', doc)\n","            \n","        normalized_corpus.append(doc)\n","        \n","    return normalized_corpus"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2c5hDSNwyTw"},"source":["### Pre-process and normalize news articles"]},{"cell_type":"code","metadata":{"id":"1da9VaH5wyTx"},"source":["news_df['full_text'] = news_df[\"news_headline\"].map(str)+ '. ' + news_df[\"news_article\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rVZRnQVBwyTz"},"source":["news_df['clean_text'] = normalize_corpus(news_df['full_text'])\n","norm_corpus = list(news_df['clean_text'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NzEFnNeuJlIo"},"source":["news_df.iloc[1][['full_text', 'clean_text']].to_dict()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MpmBJJXhwyT2"},"source":["### Save the news articles"]},{"cell_type":"code","metadata":{"id":"NLVKFz3RwyT3"},"source":["news_df.to_csv('news.csv', index=False, encoding='utf-8')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fPr-C-Q1n3pE"},"source":["### Tokenization and Vectorization"]},{"cell_type":"markdown","metadata":{"id":"jbpf2btMoP4B"},"source":["TODO: Split data between training and text"]},{"cell_type":"code","metadata":{"id":"aTTHBLOun7Bq"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sSIK2ZYfn-uE"},"source":["#Using it with regular text\n","vect = TfidfVectorizer()\n","vect.fit(news_df['clean_text'].tolist())\n","len(vect.get_feature_names())"],"execution_count":null,"outputs":[]}]}